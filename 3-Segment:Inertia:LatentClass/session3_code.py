'''Foundation of Advanced Quantitative MarketingSeesion 3:     1) Observable heterogeneity models    2) State-dependence models    3) Latent class logit model.Data: Yogurt_Data.csvAuthor: Jingpeng HongInstructor: Pradeep Chintagunta'''#################### Housekeeping ####################import osimport pandas as pdimport numpy as npimport scipy as spfrom statsmodels.discrete.conditional_models import ConditionalLogit as clfrom statsmodels.iolib.summary2 import summary_colos.chdir("/Users/hongjingpeng/Desktop/Quant_Mkt/Advanced-Quantitative-Marketing-2022W/")###################### Preparing Data ######################## load original data ##df = pd.read_csv("Data/Yogurt_Data.csv", usecols=range(19))## reshape data from wide to long ##df.rename(columns = {'Brand  2': 'Brand 2'}, inplace = True)colname = df.columns.tolist()df2 = pd.wide_to_long(df, stubnames=["Brand", "Feature", "Price"], i=colname[0:6], j="brand", sep=" ")df2 = df2.reset_index()df2.rename(columns = {'Brand': 'choice'}, inplace = True)## brand dummy ##brand_dummy = pd.get_dummies(df2['brand'], prefix='brand')df2 = pd.concat([df2, brand_dummy], axis=1)## shopping occation ##nJ = df2['brand'].nunique() # number of choicesnN = int(len(df2)/nJ) # number of purchase occasionst = np.tile(np.arange(nN), nJ).reshape((nJ, nN)).T.reshape(-1) # purchase occasion groupdf2['t'] = t############################# Observed Heterogenity ################################################# Median Split ###################### construct household characteristic data framedf_hh = df[['Pan I.D.', 'Income', 'HH Size']].drop_duplicates().reset_index()del df_hh['index']## median splitmed_size, med_income = df_hh[['HH Size', 'Income']].median()df2['upper_size'] = df2['HH Size'] > med_sizedf2['upper_income'] = df2['Income'] > med_incomedf2.loc[(df2['upper_size'] == True) & (df2['upper_income'] == True), 'group'] = 1df2.loc[(df2['upper_size'] == True) & (df2['upper_income'] == False), 'group'] = 2df2.loc[(df2['upper_size'] == False) & (df2['upper_income'] == True), 'group'] = 3df2.loc[(df2['upper_size'] == False) & (df2['upper_income'] == False), 'group'] = 4## conditional logit estimation est_list = []for i in range(1, 5):    df_sg = df2.loc[df2['group'] == i, ]    x = df_sg[['brand_1', 'brand_2', 'brand_3', 'Price', 'Feature']]    nN_sg = int(len(df_sg)/nJ) # number of households    t = np.tile(np.arange(nN_sg), nJ).reshape((nJ, nN_sg)).T # purchase occasion group    est = cl(endog = df_sg.choice.to_numpy(),                 exog = x.to_numpy(),                 groups = t.reshape(-1)).fit()    est_list.append(est)res = summary_col([est_list[0],est_list[1], est_list[2], est_list[3]])res.tables[0].to_csv("output/seglogit.csv")#################################### Demographics as Interactions ###################################### generate interaction termsinter_brand = []brand_x = [df2['brand_1'], df2['brand_2'], df2['brand_3'],            df2['Price'], df2['Feature']]for x in brand_x:    inter1 = x * df2['Income']    inter_brand.append(inter1)    inter2 = x * df2['HH Size']    inter_brand.append(inter2)inter_brand = pd.DataFrame(inter_brand).Tinter_brand.columns = ['I1', 'HH1', 'I2', 'HH2', 'I3', 'HH3',                       'Ip', 'HHp', 'If', 'HHf']df2 = pd.concat([df2, inter_brand], axis=1)## model estimationx = df2[['brand_1', 'brand_2', 'brand_3', 'Price', 'Feature',         'Ip', 'HHp', 'If', 'HHf']]nN = int(len(df2)/nJ) # number of householdst = np.tile(np.arange(nN), nJ).reshape((nJ, nN)).T # purchase occasion groupest = cl(endog = df2.choice.to_numpy(),          exog = x.to_numpy(),          groups = t.reshape(-1)).fit()summary_col(est).tables[0].to_csv("output/interaction_logit.csv")################################ Latent Class Logit Model ################################def LatentClassLogit(w, *args):    '''    Latent Class Logit Model    w:         s by k+1 coefficient matrix        nS: number of segments        nK: number of attributes, the first is the probability within the segment    '''        data = args[0]        ## possibility within segment    w_arr = np.array(w)    w_arr = np.reshape(w_arr, (-1,6))    lam = w_arr[:, 0] # segment probability    pi = np.exp(lam)/sum(np.exp(lam))    para = w_arr[:, 1:] # other coefficients        # nS, number of segments; nK, number of attributes    nS, nK = np.shape(w_arr)         # brand specofic characteristics    attribute = data[['brand_1', 'brand_2', 'brand_3', 'Feature', 'Price']]    # utility and probability    u = attribute @ para.T    expu = np.exp(u)    t = pd.DataFrame(np.tile(np.arange(nN), nJ).reshape((nJ, nN)).T.reshape(-1))    t.columns = ['t']    exp = pd.concat([t, expu], axis = 1)    exp_sum = exp.groupby('t')[list(range(nS))].agg('sum')    exp_sum_dup = np.tile(exp_sum, nJ).reshape(nJ*len(exp_sum), nS)    p = expu / exp_sum_dup    data = pd.concat([data, p], axis = 1)        data.loc[(data['choice'] == 0), p.columns] = 1    L = data.groupby('Pan I.D.', as_index = False)[p.columns].prod()    Li = L.iloc[:,1:] @ pi    return -sum(np.log(Li))# 1 segmentresult1 = sp.optimize.minimize(LatentClassLogit, [[0]*6], args=(df2), method='L-BFGS-B')coef1 = result1.xprint(coef1)lam = np.reshape(coef1, (-1,6))[:, 0] # segment probabilitypi = np.exp(lam)/sum(np.exp(lam))print(pi)hess_inv=result1.hess_inv.todense() print(np.diag(np.sqrt(hess_inv)))BIC1 = 2 * LatentClassLogit(coef1, df2) + np.log(nN) * 5print(BIC1)# 2 segmentresult2 = sp.optimize.minimize(LatentClassLogit, [[1]*6, [2]*6], args=(df2), method='L-BFGS-B')coef2 = result2.xprint(coef2)lam = np.reshape(coef2, (-1,6))[:, 0] # segment probabilitypi = np.exp(lam)/sum(np.exp(lam))print(pi)hess_inv=result2.hess_inv.todense() print(np.diag(np.sqrt(hess_inv)))BIC2 = 2 * LatentClassLogit(coef2, df2) + np.log(nN) * 12print(BIC2)# 3 segmentresult3 = sp.optimize.minimize(LatentClassLogit, [[1]*6, [2]*6, [3]*6], args=(df2), method='L-BFGS-B')coef3 = result3.xprint(coef3)lam = np.reshape(coef3, (-1,6))[:, 0] # segment probabilitypi = np.exp(lam)/sum(np.exp(lam))print(pi)hess_inv=result3.hess_inv.todense() print(np.diag(np.sqrt(hess_inv)))BIC3 = 2 * LatentClassLogit(coef3, df2) + np.log(nN) * 18print(BIC3)# 4 segmentresult4 = sp.optimize.minimize(LatentClassLogit, [[1]*6, [2]*6, [3]*6, [4]*6], args=(df2), method='L-BFGS-B')coef4 = result4.xprint(coef4)lam = np.reshape(coef4, (-1,6))[:, 0] # segment probabilitypi = np.exp(lam)/sum(np.exp(lam))print(pi)hess_inv=result4.hess_inv.todense() print(np.diag(np.sqrt(hess_inv)))BIC4 = 2 * LatentClassLogit(coef4, df2) + np.log(nN) * 24print(BIC4)# 5 segmentresult5 = sp.optimize.minimize(LatentClassLogit, [[1]*6, [2]*6, [3]*6, [4]*6, [5]*6], args=(df2), method='L-BFGS-B')coef5 = result5.xprint(coef5)lam = np.reshape(coef5, (-1,6))[:, 0] # segment probabilitypi = np.exp(lam)/sum(np.exp(lam))print(pi)hess_inv=result5.hess_inv.todense() print(np.diag(np.sqrt(hess_inv)))BIC5 = 2 * LatentClassLogit(coef5, df2) + np.log(nN) * 30print(BIC5)